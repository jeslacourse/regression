[["simple-linear-regression.html", "Regression Analysis 1 Simple Linear Regression 1.1 Covariance 1.2 Correlation 1.3 Modeling Simple Linear Regression 1.4 Least Squares Estimation", " Regression Analysis Jes LaCourse 2022-09-14 1 Simple Linear Regression 1.1 Covariance Given the random variables \\((X,Y)\\) covariance is defined as: \\[cov(X,Y) = E[(X -\\mu_x)(Y-\\mu_y)]\\] with sample covariance \\[\\hat{cov}(X,Y) = \\frac{\\Sigma(X_i - \\bar{X})(Y_i-\\bar{Y})^2}{n-1} \\] 1.2 Correlation Correlation is defined on the scale \\(-1 \\leq corr(X,Y) \\leq 1\\) where: \\[corr(X,Y) = \\frac{cov(X,Y)}{\\sqrt{var(X)var(Y)}}\\] 1.2.1 Rules for Expectation [Roll into prior post] Given random variables \\(X\\) and \\(Y\\) and constants \\(a,b,\\) &amp; \\(c\\): \\(E(c) = c\\) \\(E(cX) = cE(X)\\) \\(E(X+Y) = E(X) + E(Y)\\) An example: \\[E(a+bX+cY) \\\\ = a + bE(X) + cE(Y)\\] 1.2.2 Rules for Variance \\(var(a) = 0\\) \\(var(aX) = a^2 var(X)\\) \\(var(X+Y) = var(X) + var(Y) + 2cov(X,Y)\\) An example: \\[var(a+bX-cY) \\\\= var(a) + var(bX-cY) + 2cov(X,Y)\\\\= 0 + var(bX)+ var(-cY) \\\\=b^2var(X) + c^2var(Y)+2[(-bc)cov(X,Y)]\\] 1.2.3 Rules for Covariance \\(cov(X,Y) = cov(Y,X)\\) if \\(X  Y \\Rightarrow cov(X,Y) = 0\\), if the random variables are independent of one another, then we cannot predict their variability if \\(cov(X,Y)= 0 \\Rightarrow\\) may or may not \\(X  Y\\) \\(cov(a,b) = 0\\) \\(cov(a, X) = 0\\) \\(cov(aX, Y) = a cov(X,Y)\\) \\(cov(aX, bY) = ab cov(X,Y)\\) \\(cov(X, Y+Z) = cov(X,Y)+cov(X,Z)\\) \\(cov(X,X) = var(X)\\) 1.3 Modeling Simple Linear Regression \\[Y_i= \\beta_0 +\\beta_1X_i + \\epsilon_i\\] where: \\(Y_i\\): random variable for experimental unit \\(X_i\\): predictor for experimental unit, a fixed value \\(\\beta_0, \\beta_1\\): parameters, usually known. The goal is to estimate them. The model is considered simple because we are using one predictor. The model is linear in parameters and linear in the predictor variable, \\(X\\). Properties for \\(\\epsilon_i\\): 1. \\(E(\\epsilon_i) = 0\\), \\(\\forall i\\) 2. \\(var(\\epsilon_i)\\) is constant 3. \\(\\epsilon_i...\\epsilon_n\\) are uncorrelated On average, the error is the regression line, 0. Put another way, The expected value of \\(\\epsilon\\) is on the regression line. \\[ E(\\epsilon_i) = 0\\\\ \\epsilon_i = Y_i -(\\beta_0 +\\beta_1X_i) \\\\ E[\\epsilon_i] = E[Y_i -(\\beta_0 +\\beta_1X_i)] \\\\ 0 = E[Y_i] - E[\\beta_0 +\\beta_1X_i] \\\\ 0 = E[Y_i] - E[\\beta_0] +E[\\beta_1X_i] \\\\ 0 = E[Y_i] - \\beta_0 + \\beta_1X_i \\\\ \\Rightarrow E[Y_i] = \\beta_0 + \\beta_1X_i \\\\ \\] showing that on average, \\(Y_i\\) falls along the regression line. We can say the regression lineis the average of \\(Y_i\\), conditional on \\(X_i\\). \\[ E(Y_i|X_i)= \\beta_0 +\\beta_1X_i \\] 2. \\(\\epsilon_i...\\epsilon_n\\) have the same variance Define $^2(_i) ^2 $, then $^2(Y_i) = var(Y_i) $. \\(var(\\epsilon_i)\\) increases as \\(X\\) increases. \\(\\epsilon_i...\\epsilon_n\\) are uncorrelated \\[ corr(\\epsilon_i,\\epsilon_j) = 0, i\\neq j \\\\ corr(\\epsilon_i,\\epsilon_j) = var(\\epsilon_i) \\neq 0 \\\\ cov(\\epsilon_i,\\epsilon_j) = 0, i\\neq j \\] Similarly, but with different notation, \\(corr(\\epsilon_i,\\epsilon_j) = \\sigma(\\epsilon_i,\\epsilon_j)\\). 1.3.1 Features of Models \\(Y_i\\) is a sum of a constant term plus a random variable. The constant in this case is \\(\\beta_0 +\\beta_1X_i\\). The random variable is \\(\\epsilon_i\\). \\(E(Y_i|X_i)= \\beta_0 +\\beta_1X_i\\), see \\(\\epsilon\\) property 2. \\(\\epsilon_i = Y_i -(\\beta_0 +\\beta_1X_i)\\) is a random deviation of \\(Y_i\\) around the regression line. Its worth taking a moment to make the distinction between the regression model, which uses subscripts \\(i\\): \\[Y_i= \\beta_0 +\\beta_1X_i + \\epsilon_i\\] and the regression function: \\[Y= \\beta_0 +\\beta_1X + \\epsilon\\] An example: We want to estimate the systolic blood pressure for a 20 year old. We know the following variables: \\(x\\): age \\(y\\): systolic blood pressure \\(\\beta_0\\): the intercept, \\(90\\) \\(\\beta_1\\): the slope, \\(0.9\\) Note that \\(\\beta\\)s are usually unknown. We can say, we expect systolic blood pressure to be \\(90\\) at age \\(0\\) (or at birth), increasing \\(0.9\\) units every year. \\[ E(Y|X = 20) \\\\= 90 + 0.9(20)\\\\=108 \\] Factoring in an error margin, given \\(X = 20\\) we would expect \\(Y = 108 +\\epsilon\\). 1.4 Least Squares Estimation 1.4.1 Estimating \\(\\beta\\)s Find good estimates of \\(\\beta_0\\) and \\(\\beta_1\\) using the least squares method. When you have \\(n\\) pairs of data, \\((x_1, y_1)...(x_n, y_n)\\): Use the deviation formula \\[e_i = y_i - (\\beta_0 +\\beta_1x_i)\\] to compose the squared deviation formula: \\[e_i^2 = [y_i - (\\beta_0 +\\beta_1x_i)]^2\\] This will allow us to define \\(Q\\), the sum of squared deviations: \\[ Q \\equiv \\Sigma_{i = 1}^n e_i^2\\\\=\\Sigma_{i = 1}^n [y_i - (\\beta_0 +\\beta_1x_i)]^2 \\] The least squares estimates are values of \\(\\beta_0\\) and \\(\\beta_1\\) that minimize \\(Q\\). We use \\(b_1\\) to estimate the slope \\(\\beta_1\\): $$ b_1 = \\ = $$ Once we have \\(b_1\\), the estimate for the intercept \\(\\beta_0\\) is straightforward: \\[ b_0 = \\bar{y}- b_1\\bar{x}\\\\ \\bar{y} = b_0 + b_1\\bar{x} \\] where the point \\((\\bar{x}, \\bar{y})\\) falls along the regression line. Properties The estimators \\(b_0\\) and \\(b_1\\) are unbiased estimators. They only require that for any \\(\\epsilon_i\\), that \\(E(\\epsilon_i) = 0\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
