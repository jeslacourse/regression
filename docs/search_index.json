[["index.html", "Regression Analysis Preface", " Regression Analysis Jes LaCourse 2022-09-20 Preface Notebook in progress "],["simple-linear-regression.html", "1 Simple Linear Regression 1.1 Modeling 1.2 Utility 1.3 Expected Values 1.4 Variance 1.5 Bias 1.6 Covariance 1.7 Correlation 1.8 Modeling Simple Linear Regression 1.9 Least Squares Estimation", " 1 Simple Linear Regression One predictor, one outcome Linear Regression: The measure of lineal relationship between two variables. Simple regression will have one continuous response variable (y) and one continuous explanatory variable (x). Response variables are also known as the dependent or outcome variable. Explanatory variables, aka independent, predictor, or covariate variables, can include categorical values. \\[y=\\beta_0 + \\beta_1x\\] Where \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is the slope of the function. 1.1 Modeling \\(X\\) is treated as a fixed variable whose values have been chosen by the researcher. Though it should be noted that regression is often used where variable is not wholly chosen. For example, sampling a population will return a distribution of ages that may not match the true distribution of the population. So \\(X\\) is also, technically, random. \\(Y\\) is a random variable Our regression model dependency of \\(Y\\) on \\(X\\) esti \\[Y=\\beta_0 + \\beta_1X + \\epsilon\\] \\(\\beta\\)s and \\(X\\) are considered fixed values, with random variable \\(\\epsilon\\). Where \\(\\beta_0\\) and \\(\\beta_1\\) are parameters; usually unknown values related to the population and not the sample. parameter: values for the population statistic: values for the sample We are using statistics to estimate our parameters After collecting data from the population, we can create a sample on which to run our statistics for which we can estimate our parameters. parameter statistic mean \\(\\mu\\) \\(\\bar{y}\\) variance \\(\\sigma^2\\) \\(s^2\\) slope \\(\\beta_1\\) \\(b_1\\) intercept \\(\\beta_0\\) \\(b_0\\) 1.2 Utility Regression is used for: Observational studies just observe, no manipulation treatment is not randomized Experimental studies manipulate the explanatory variables treatment must be randomized (!) Correlations is not equal to causation in non-random studies 1.3 Expected Values For a discrete random variable \\(Y\\) with possible values \\(y_1... y_k\\) we can say: \\[ E(Y) = \\Sigma_{i=1}^k y_1 P(Y = y_i)\\] where \\(E(Y)\\) is a weighted average of the possible values \\(y_1... y_k\\) and the weights \\(P(Y = y_1)...P(Y = y_k)\\) are the expected probabilities. This will give us a weighted average. Weighted Average: \\[\\Sigma_{i=1}^k w_ia_i\\] If \\(\\Sigma w_i = 1\\) and \\(0 \\leq w_1 \\leq 1\\). A continuous random variable \\(Y\\) would be represented by a density function: \\[E(Y) = \\int_{-\\infty}^{\\infty} yf(y)dy\\] For example, the normal distribution Y ~ N(0,1) with mean 0 and variance 1, would be represented using the density function: \\[E(Y) = \\int_{-\\infty}^{\\infty}y\\frac{1}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{y^2}{2}\\right) dy = 0 \\] The population mean is the expected value. If \\(Z_1..Z_N\\) for all values in the population, then we would use the parameter: \\[ E(Z) = \\frac{1}{N}\\Sigma_{i=1}^N Z_i\\] Sample means are averages. Given the sample data \\(X_1... X_n\\), would return the statistic: \\[ \\bar{x} = \\frac{1}{n} \\Sigma_{i = 1}^nX_i\\] 1.3.1 Rules for Expectation Given random variables \\(X\\) and \\(Y\\) and constants \\(a,b,\\) &amp; \\(c\\): \\(E(c) = c\\) \\(E(cX) = cE(X)\\) \\(E(X+Y) = E(X) + E(Y)\\) An example: \\[E(a+bX+cY) \\\\ = a + bE(X) + cE(Y)\\] 1.4 Variance We can use the population variance formula to show that \\(E(Y)\\) is equivalent to the parameter \\(\\mu\\). Given the population variance \\(var(Y)\\): \\[ var(Y) = E[Y-E(Y)]^2\\\\=E[Y-\\mu]^2\\\\\\therefore \\mu = E(Y)\\] The squaring of \\([Y-E(Y)]\\) removes the sign, reverting all distances to positive distances from the mean. Given our known substitutions, we can say: \\[ var(Y) = E[Y-E(Y)]^2\\\\=\\Sigma_{i=1}^k (y_i-\\mu)^2P(Y-y_i)\\] If \\(Z_1... Z_n\\) account for all values in a population, we would have parameter: \\[var(Z) = \\frac{1}{N}\\Sigma_{i=1}^{N}(Z_i-\\bar{Z})^2\\] where \\(\\bar{Z} = E(Z)\\). Sample data \\(x_i...x_n\\) would give us the unbiased sample variance: \\[s^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n-1}\\] Focusing on the denominator, sample sizes \\(n\\) will shift the statistic significantly compared to if the sample was larger. 1.4.1 Rules for Variance \\(var(a) = 0\\) \\(var(aX) = a^2 var(X)\\) \\(var(X+Y) = var(X) + var(Y) + 2cov(X,Y)\\) An example: \\[var(a+bX-cY) \\\\= var(a) + var(bX-cY) + 2cov(X,Y)\\\\= 0 + var(bX)+ var(-cY) \\\\=b^2var(X) + c^2var(Y)+2[(-bc)cov(X,Y)]\\] 1.5 Bias Because of random variability, \\(s^2\\) will vary with each experiment. An unbiased sample variance is an average value of those experiments. \\[ E(s^2) = E\\frac{\\Sigma(Y_i-\\bar{Y})^2}{n-1}= var(Y)\\] 1.6 Covariance Given the random variables \\((X,Y)\\) covariance is defined as: \\[cov(X,Y) = E[(X -\\mu_x)(Y-\\mu_y)]\\] with sample covariance \\[\\hat{cov}(X,Y) = \\frac{\\Sigma(X_i - \\bar{X})(Y_i-\\bar{Y})^2}{n-1} \\] ### Rules for Covariance \\(cov(X,Y) = cov(Y,X)\\) if \\(X  Y \\Rightarrow cov(X,Y) = 0\\), if the random variables are independent of one another, then we cannot predict their variability if \\(cov(X,Y)= 0 \\Rightarrow\\) may or may not \\(X  Y\\) \\(cov(a,b) = 0\\) \\(cov(a, X) = 0\\) \\(cov(aX, Y) = a cov(X,Y)\\) \\(cov(aX, bY) = ab cov(X,Y)\\) \\(cov(X, Y+Z) = cov(X,Y)+cov(X,Z)\\) \\(cov(X,X) = var(X)\\) 1.7 Correlation Correlation is defined on the scale \\(-1 \\leq corr(X,Y) \\leq 1\\) where: \\[corr(X,Y) = \\frac{cov(X,Y)}{\\sqrt{var(X)var(Y)}}\\] 1.8 Modeling Simple Linear Regression \\[Y_i= \\beta_0 +\\beta_1X_i + \\epsilon_i\\] where: \\(Y_i\\): random variable for experimental unit \\(X_i\\): predictor for experimental unit, a fixed value \\(\\beta_0, \\beta_1\\): parameters, usually known. The goal is to estimate them. The model is considered simple because we are using one predictor. The model is linear in parameters and linear in the predictor variable, \\(X\\). Properties for \\(\\epsilon_i\\): 1. \\(E(\\epsilon_i) = 0\\), \\(\\forall i\\) 2. \\(var(\\epsilon_i)\\) is constant 3. \\(\\epsilon_i...\\epsilon_n\\) are uncorrelated On average, the error is the regression line, 0. Put another way, The expected value of \\(\\epsilon\\) is on the regression line. \\[ E(\\epsilon_i) = 0\\\\ \\epsilon_i = Y_i -(\\beta_0 +\\beta_1X_i) \\\\ E[\\epsilon_i] = E[Y_i -(\\beta_0 +\\beta_1X_i)] \\\\ 0 = E[Y_i] - E[\\beta_0 +\\beta_1X_i] \\\\ 0 = E[Y_i] - E[\\beta_0] +E[\\beta_1X_i] \\\\ 0 = E[Y_i] - \\beta_0 + \\beta_1X_i \\\\ \\Rightarrow E[Y_i] = \\beta_0 + \\beta_1X_i \\\\ \\] showing that on average, \\(Y_i\\) falls along the regression line. We can say the regression lineis the average of \\(Y_i\\), conditional on \\(X_i\\). \\[ E(Y_i|X_i)= \\beta_0 +\\beta_1X_i \\] 2. \\(\\epsilon_i...\\epsilon_n\\) have the same variance Define $^2(_i) ^2 $, then $^2(Y_i) = var(Y_i) $. \\(var(\\epsilon_i)\\) increases as \\(X\\) increases. \\(\\epsilon_i...\\epsilon_n\\) are uncorrelated \\[ corr(\\epsilon_i,\\epsilon_j) = 0, i\\neq j \\\\ corr(\\epsilon_i,\\epsilon_j) = var(\\epsilon_i) \\neq 0 \\\\ cov(\\epsilon_i,\\epsilon_j) = 0, i\\neq j \\] Similarly, but with different notation, \\(corr(\\epsilon_i,\\epsilon_j) = \\sigma(\\epsilon_i,\\epsilon_j)\\). 1.8.1 Features of Models \\(Y_i\\) is a sum of a constant term plus a random variable. The constant in this case is \\(\\beta_0 +\\beta_1X_i\\). The random variable is \\(\\epsilon_i\\). \\(E(Y_i|X_i)= \\beta_0 +\\beta_1X_i\\), see \\(\\epsilon\\) property 2. \\(\\epsilon_i = Y_i -(\\beta_0 +\\beta_1X_i)\\) is a random deviation of \\(Y_i\\) around the regression line. Its worth taking a moment to make the distinction between the regression model, which uses subscripts \\(i\\): \\[Y_i= \\beta_0 +\\beta_1X_i + \\epsilon_i\\] and the regression function: \\[Y= \\beta_0 +\\beta_1X + \\epsilon\\] An example: We want to estimate the systolic blood pressure for a 20 year old. We know the following variables: \\(x\\): age \\(y\\): systolic blood pressure \\(\\beta_0\\): the intercept, \\(90\\) \\(\\beta_1\\): the slope, \\(0.9\\) Note that \\(\\beta\\)s are usually unknown. We can say, we expect systolic blood pressure to be \\(90\\) at age \\(0\\) (or at birth), increasing \\(0.9\\) units every year. \\[ E(Y|X = 20) \\\\= 90 + 0.9(20)\\\\=108 \\] Factoring in an error margin, given \\(X = 20\\) we would expect \\(Y = 108 +\\epsilon\\). 1.9 Least Squares Estimation 1.9.1 Estimating \\(\\beta\\)s Find good estimates of \\(\\beta_0\\) and \\(\\beta_1\\) using the least squares method. When you have \\(n\\) pairs of data, \\((x_1, y_1)...(x_n, y_n)\\): Use the deviation formula \\[e_i = y_i - (\\beta_0 +\\beta_1x_i)\\] to compose the squared deviation formula: \\[e_i^2 = [y_i - (\\beta_0 +\\beta_1x_i)]^2\\] This will allow us to define \\(Q\\), the sum of squared deviations: \\[ Q \\equiv \\Sigma_{i = 1}^n e_i^2\\\\=\\Sigma_{i = 1}^n [y_i - (\\beta_0 +\\beta_1x_i)]^2 \\] The least squares estimates are values of \\(\\beta_0\\) and \\(\\beta_1\\) that minimize \\(Q\\). We use \\(b_1\\) to estimate the slope \\(\\beta_1\\): $$ b_1 = \\ = $$ Once we have \\(b_1\\), the estimate for the intercept \\(\\beta_0\\) is straightforward: \\[ b_0 = \\bar{y}- b_1\\bar{x}\\\\ \\bar{y} = b_0 + b_1\\bar{x} \\] where the point \\((\\bar{x}, \\bar{y})\\) falls along the regression line. Properties The estimators \\(b_0\\) and \\(b_1\\) are unbiased estimators. They only require that for any \\(\\epsilon_i\\), that \\(E(\\epsilon_i) = 0\\). So, "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
