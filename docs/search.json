[{"path":"index.html","id":"section","chapter":"","heading":"","text":"Notebook progressWith help Bookdown, Rmarkdown Cookbook, little Office Ipsum.","code":""},{"path":"index.html","id":"general-content","chapter":"","heading":"0.0.1 General Content","text":"’s mixter general .SummaryThese summary notes thoughts start chapter. Use .recap tag optional .centered header.really like colour can change , make sexy, animation work, print page. best can can black darker jazz little, ideal world like , can snow look little warmer royalties company instead cash.test material. Use testmaterial flag. Latex boxes tend built-padding, expect big ol’ boxes like :\\[\ny = mx+b\n\\]’ll know see sandwich needs playful, yet think need start scratch sandwich needs playful, thats saw head hamburger menu needs , totally different . Can run inline? Remember , grade depends . Yeah works . Can black darker can retro, jazz little. red red according brief. work us “pro bono” really add portfolio promise ’ll know see actual logo instead font, don’t need backup, never goes !Office ipsum gives anxiety sometimes. Use [Remember ]{class=\"tm-inline\"} inline boxes.","code":""},{"path":"index.html","id":"examples","chapter":"","heading":"0.0.2 Examples","text":"color: Warning please, please help make sense monster practical applications.sample problem. may mix plain text LaTex. Flexboxes multi paragraph sections don’t play nice now. Use .callout .callout-warning tags.\\[\ny-y_0 = m(x-x_0)\n\\]\nAnyway, designer, know can help ? get lot free exposure can remove double chin business card photo? don’t like way looks yet low resolution? looks ok screen. Can use high definition screenshot can handle million one go, agencies charge much lesser, lucky even us, actual logo instead font. Can please send design specs ?color: Success Hashtag winning? Two examples better one, guess. Though expect use color infrequently. ’s great color, though.another sample problem. may mix plain text LaTex. Use .callout .callout-success tags.\\[\ny-x_0 \\neq m(x-y_0)\n\\]\nlove , can invert colors? just think. trust , ’ll know see . got invoice…seems really high.","code":""},{"path":"index.html","id":"definitions","chapter":"","heading":"0.0.3 Definitions","text":"color: Info Oh hey, look. Purple LaTex! inline badges! Pretty proud one.\\[\ny = x^2 + x - 1\n\\]chance can get color inline? Yeah, certainly. Use square-curly bracket combination, [definition]{class=\"text-info\"}.definition callout. ’s even little icon, fancy . Icons play nice ’s one paragraph material. block LaTex break paragraph.","code":""},{"path":"index.html","id":"dangers","chapter":"","heading":"0.0.4 Dangers","text":"callout danger text. printed , animated gif moving. big name portfolio don’t need contract, can make pop. needs , totally different can handle million one go.","code":""},{"path":"simple-linear-regression.html","id":"simple-linear-regression","chapter":"1 Simple Linear Regression","heading":"1 Simple Linear Regression","text":"One predictor, one outcomeIn SummaryThis section serves introduction expectation, variance, bias, modeling. topics serve building blocks linear regression. sample re-sample data repeated, ’ll start see statistics, estimates, start mirror true parameters emulate.Linear Regression: measure lineal relationship two variables.Regression used observational studies experimental studies. Observational studies experiments manipulation researcher. Treatments randomized. Experimental studies allow researcher manipulate explanatory variables. treatment data must randomized.","code":""},{"path":"simple-linear-regression.html","id":"variables","chapter":"1 Simple Linear Regression","heading":"1.1 Variables","text":"Simple regression one continuous response variable (\\(y\\)) one continuous explanatory variable (\\(x\\)). response variable, \\(y\\) also known dependent outcome variable. Explanatory variables, \\(x\\) aka independent, predictor, covariate variables, can include categorical values. also two parameters regression: \\(\\beta_0\\) y-intercept \\(\\beta_1\\) slope function. four components make linear regression equation, variant slope-intercept form.\\[y=\\beta_0 + \\beta_1x\\]Fixed Random VariablesFixed variables don’t change one experiment next. values chosen researcher. \\(X\\) treated fixed variable regression. said, regression often used \\(X\\) wholly chosen; may simply data work . example, sampling population return distribution ages may match true distribution population. treat \\(X\\) fixed value, also technically random.Random variables expected return different values repeating experiment. variability true regression line points \\((X,Y)\\) captured error value, \\(\\epsilon\\).\\[\nY=\\beta_0 + \\beta_1X + \\epsilon\n\\]\n\\(\\beta\\)’s considered fixed values along \\(X\\), whereas \\(\\epsilon\\) random variable.\\(Y\\) also random variable given ’s dependency \\(X\\) effects \\(\\epsilon\\).Parameters StatisticsStatistics used estimate true parameters\\(\\beta_0\\) \\(\\beta_1\\) parameters; usually unknown values related population sample.Parameter: values populationStatistic: values sampleAfter collecting data population, can create sample run statistics can estimate parameters.","code":""},{"path":"simple-linear-regression.html","id":"expected-values","chapter":"1 Simple Linear Regression","heading":"1.2 Expected Values","text":"Expected values mean average values. look little different depending data looks like.Sample means averages. Given sample data \\(X_1... X_n\\), return statistic:\\[ \\bar{x}  = \\frac{1}{n} \\sum_{= 1}^nX_i\\]several repeated experiments, can come population mean, expected value. \\(Z_1..Z_N\\) values population, use parameter:\\[ E(Z) = \\frac{1}{N}\\sum_{=1}^N Z_i\\]general, discrete random variable \\(Y\\) possible values \\(y_1... y_k\\) can say \\(E(Y)\\) weighted average possible values \\(y_1... y_k\\) respective expected probabilities \\(P(Y = y_1)...P(Y = y_k)\\):\\[\\begin{equation} E(Y) = \\sum_{=1}^k y_i P(Y = y_i)\\end{equation}\\]Expectation discrete values modification weighted average formula. Given limitations probabilities, simply add caveats sum probabilities equal 100%, \\(\\sum w_i = 1\\), probability can reasonably happen. can’t 200% chance rain, right? limit 0-100% probability, \\(0 \\leq w_1 \\leq 1\\).\\[\\begin{equation}\\sum_{=1}^k w_ia_i\\end{equation}\\]Expectation continuous random variable \\(Y\\) calculated density function:\\[E(Y) = \\int_{-\\infty}^{\\infty} yf(y)dy\\]\n’s thing, yet terrible integrals.Example normal distribution Y ~ N(0,1) mean 0 variance 1, represented using density function:\\[E(Y) = \\int_{-\\infty}^{\\infty}y\\frac{1}{\\sqrt{2\\pi}}\n  \\exp\\left( -\\frac{y^2}{2}\\right) dy = 0 \\]","code":""},{"path":"simple-linear-regression.html","id":"rules-for-expectation","chapter":"1 Simple Linear Regression","heading":"1.2.1 Rules for Expectation","text":"Given random variables \\(X\\) \\(Y\\) constants \\(,b,\\) & \\(c\\):\\(E(c) = c\\)\\(E(cX) = cE(X)\\)\\(E(X+Y) = E(X) + E(Y)\\)Example\\[\\begin{align}   \nE(+bX+cY) \\\\\n= + (X) + cE(Y)\n\\end{align}\\]","code":""},{"path":"simple-linear-regression.html","id":"bias","chapter":"1 Simple Linear Regression","heading":"1.3 Bias","text":"estimators \\(b_0\\) \\(b_1\\) unbiased estimators. require \\(\\epsilon_i\\), \\(E(\\epsilon_i) = 0\\).\\(E(b_0) =\\beta_0\\) \\(E(b_1) = \\beta_1\\).Bias term refers much estimate (estimator) true value. Contextually, want know far slope intercept sample values experiment deviates true regression line.random variability, \\(s^2\\) vary experiment. repeating experiment can collect unbiased sample variance, average value experiments.\\[ E(s^2) = E\\frac{\\sum(Y_i-\\bar{Y})^2}{n-1}= var(Y)\\]","code":""},{"path":"simple-linear-regression.html","id":"variance-and-covariance","chapter":"1 Simple Linear Regression","heading":"1.4 Variance and Covariance","text":"","code":""},{"path":"simple-linear-regression.html","id":"variance","chapter":"1 Simple Linear Regression","heading":"1.4.1 Variance","text":"Sample data \\(x_i...x_n\\) give us unbiased sample variance:\\[s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\\]Focusing denominator, sample sizes \\(n\\) shift statistic significantly compared larger sample.Population variance can defined :\n:::{.text-info}\n\\[\nvar(Y) = E[Y-E(Y)]^2\n\\]\n:::squaring \\([Y-E(Y)]\\) “removes sign”, reverts measures positive distances mean.\ncan use population variance formula show \\(E(Y)\\) equivalent parameter \\(\\mu\\). Given \\(var(Y)\\):\\[\\begin{align} var(Y) = E[Y-E(Y)]^2\\\\=E[Y-\\mu]^2\\\\\\therefore \\mu = E(Y)\\end{align}\\]Given known substitutions, can say:\\[\\begin{align} var(Y)= E[Y-E(Y)]^2\\\\\n=\\sum_{=1}^k (y_i-\\mu)^2P(Y-y_i)\n\\end{align}\\]\\(Z_1... Z_n\\) account values population, parameter:\\[var(Z) = \\frac{1}{N}\\sum_{=1}^{N}(Z_i-\\bar{Z})^2\\]\\(\\bar{Z} = E(Z)\\).Rules Variance\\(var() = 0\\)\\(var(aX) = ^2 var(X)\\)\\(var(X+Y) = var(X) + var(Y) + 2cov(X,Y)\\)Example\n\\[\\begin{align}\nvar(+bX-cY) \\\\\n= var() + var(bX-cY) + 2cov(X,Y)\\\\\n= 0 + var(bX)+ var(-cY) \\\\\n=b^2var(X) + c^2var(Y)+2[(-bc)cov(X,Y)]\n\\end{align}\\]","code":""},{"path":"simple-linear-regression.html","id":"covariance","chapter":"1 Simple Linear Regression","heading":"1.4.2 Covariance","text":"Given random variables \\((X,Y)\\) covariance defined :\\[cov(X,Y) = E[(X -\\mu_x)(Y-\\mu_y)]\\]sample covariance\\[\\hat{cov}(X,Y) = \\frac{\\sum(X_i - \\bar{X})(Y_i-\\bar{Y})^2}{n-1} \\]Rules Covariance\\(cov(X,Y) = cov(Y,X)\\)\\(X ⫫ Y \\Rightarrow cov(X,Y) = 0\\), random variables independent one another, predict variabilityif \\(cov(X,Y)= 0 \\Rightarrow\\) may may \\(X ⫫ Y\\)\\(cov(,b) = 0\\)\\(cov(, X) = 0\\)\\(cov(aX, Y) = cov(X,Y)\\)\\(cov(aX, ) = ab cov(X,Y)\\)\\(cov(X, Y+Z) = cov(X,Y)+cov(X,Z)\\)\\(cov(X,X) = var(X)\\)","code":""},{"path":"simple-linear-regression.html","id":"correlation","chapter":"1 Simple Linear Regression","heading":"1.4.3 Correlation","text":"Correlations equal causation non-random studiesCorrelation defined scale \\(-1 \\leq corr(X,Y) \\leq 1\\) :\\[corr(X,Y) = \\frac{cov(X,Y)}{\\sqrt{var(X)var(Y)}}\\]","code":""},{"path":"simple-linear-regression.html","id":"modeling","chapter":"1 Simple Linear Regression","heading":"1.5 Modeling","text":"simple linear regression model :\\[Y_i= \\beta_0 +\\beta_1X_i + \\epsilon_i\\]:\\(Y_i\\): random variable experimental unit\\(X_i\\): predictor experimental unit, fixed value\\(\\beta_0, \\beta_1\\): parameters, usually known. goal estimate .model considered simple using one predictor. model linear parameters linear predictor variable, \\(X\\).Properties \\(\\epsilon_i\\):\\(E(\\epsilon_i) = 0\\), \\(\\forall \\)\\(E(\\epsilon_i) = 0\\), \\(\\forall \\)\\(var(\\epsilon_i)\\) constant\\(var(\\epsilon_i)\\) constant\\(\\epsilon_i...\\epsilon_n\\) uncorrelated\\(\\epsilon_i...\\epsilon_n\\) uncorrelatedOn average, error regression line, 0. Put another way,expected value \\(\\epsilon\\) regression line.Property 1:$$\n\\[\\begin{align}\nE(\\epsilon_i) = 0\\\\\n\\epsilon_i = Y_i -(\\beta_0 +\\beta_1X_i) \\\\\nE[\\epsilon_i] = E[Y_i -(\\beta_0 +\\beta_1X_i)] \\\\\n0 = E[Y_i] - E[\\beta_0 +\\beta_1X_i] \\\\\n0 = E[Y_i] - E[\\beta_0] +E[\\beta_1X_i] \\\\\n0  = E[Y_i] - \\beta_0 + \\beta_1X_i \\\\\n   \\Rightarrow E[Y_i] = \\beta_0 + \\beta_1X_i \\\\\n   \n\\end{align}\\]\n$$showing average, \\(Y_i\\) falls along regression line. can say regression line average \\(Y_i\\), conditional \\(X_i\\).\\[ E(Y_i|X_i)= \\beta_0 +\\beta_1X_i \\]\nProperty 2:\\(\\epsilon_i...\\epsilon_n\\) varianceDefine $^2(_i) ^2 $, $^2(Y_i) = var(Y_i) $. \\(var(\\epsilon_i)\\) increases \\(X\\) increases.Property 3:\\(\\epsilon_i...\\epsilon_n\\) uncorrelated.\\[\n\\begin{align}\n  corr(\\epsilon_i,\\epsilon_j) = 0, \\neq j \\\\\n  corr(\\epsilon_i,\\epsilon_j) = var(\\epsilon_i) \\neq 0 \\\\\n  cov(\\epsilon_i,\\epsilon_j) = 0, \\neq j\n\\end{align}\n\\]Similarly, different notation, \\(corr(\\epsilon_i,\\epsilon_j) = \\sigma(\\epsilon_i,\\epsilon_j)\\).Property 4:\\(\\epsilon_i...\\epsilon_n\\) normal.","code":""},{"path":"simple-linear-regression.html","id":"features-of-models","chapter":"1 Simple Linear Regression","heading":"1.5.1 Features of Models","text":"\\(Y_i\\) sum constant term plus random variable. constant case \\(\\beta_0 +\\beta_1X_i\\). random variable \\(\\epsilon_i\\).\\(Y_i\\) sum constant term plus random variable. constant case \\(\\beta_0 +\\beta_1X_i\\). random variable \\(\\epsilon_i\\).\\(E(Y_i|X_i)= \\beta_0 +\\beta_1X_i\\), see \\(\\epsilon\\) property 2.\\(E(Y_i|X_i)= \\beta_0 +\\beta_1X_i\\), see \\(\\epsilon\\) property 2.\\(\\epsilon_i = Y_i -(\\beta_0 +\\beta_1X_i)\\) random deviation \\(Y_i\\) around regression line.\\(\\epsilon_i = Y_i -(\\beta_0 +\\beta_1X_i)\\) random deviation \\(Y_i\\) around regression line.’s worth taking moment make distinction regression model, uses subscripts \\(\\):\\[Y_i= \\beta_0 +\\beta_1X_i + \\epsilon_i\\]\nregression function:\\[Y= \\beta_0 +\\beta_1X + \\epsilon\\]Example want estimate systolic blood pressure 20 year old. know following variables:\\(x\\): age\\(y\\): systolic blood pressure\\(\\beta_0\\): intercept, \\(90\\)\\(\\beta_1\\): slope, \\(0.9\\)Note \\(\\beta\\)’s usually unknown. can say, expect systolic blood pressure \\(90\\) age \\(0\\) (birth), increasing \\(0.9\\) units every year.\\[\\begin{align}\nE(Y|X = 20) \\\\= 90 + 0.9(20)\\\\=108\n\\end{align}\\]Factoring error margin, given \\(X = 20\\) expect \\(Y = 108 +\\epsilon\\). 20-year-old systolic blood pressure around 108.","code":""},{"path":"simple-linear-regression.html","id":"least-squares-estimation","chapter":"1 Simple Linear Regression","heading":"1.6 Least Squares Estimation","text":"","code":""},{"path":"simple-linear-regression.html","id":"formulas","chapter":"1 Simple Linear Regression","heading":"1.6.1 Formulas","text":"True regression line:\\[y= \\beta_0 + \\beta_1x\\]Estimated regression line:\\[y = b_0 + b_1x\\]Data value:\\[y_i = b_0+b_1+x_i+e_i\\]Predicted value, estimate \\(E(Y|X_i)\\):\\[\\hat{y} = b_0+b_1x_i\\]\nResidual observation \\(\\):\\(e_i=\\) observed - predicted\\[\\begin{align}\n= y_i - \\hat y\\\\\n= y_i - (b_0 - b_1x_i)\n\\end{align}\\]","code":""},{"path":"simple-linear-regression.html","id":"estimating-betas","chapter":"1 Simple Linear Regression","heading":"1.6.2 Estimating \\(\\beta\\)’s","text":"can find good estimates \\(\\beta_0\\) \\(\\beta_1\\) using least squares method, \\(b_0\\) \\(b_1\\) unbiased least squares estimates.\\(n\\) pairs data, \\((x_1, y_1)...(x_n, y_n)\\):Use deviation formula:\\[e_i = y_i - (\\beta_0 +\\beta_1x_i)\\]\\(e_i\\) observed error residual. Compose squared deviation formula:\\[e_i^2 = [y_i - (\\beta_0 +\\beta_1x_i)]^2\\]allow us define \\(Q\\), sum squared deviations:\\[\\begin{align}\nQ \\equiv \\sum_{= 1}^n e_i^2\\\\=\\sum_{= 1}^n [y_i - (\\beta_0 +\\beta_1x_i)]^2\n\\end{align}\\]least squares estimates values \\(\\beta_0\\) \\(\\beta_1\\) minimize \\(Q\\).use \\(b_1\\) estimate slope \\(\\beta_1\\):\\[\\begin{align}\nb_1 = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2}\\\\\n=\\frac{\\hat{cov}(X,Y)}{\\hat{var}(X,Y)}\n\\end{align}\\]sampling distribution \\(b_1\\) normal.\n\\[ b_1\\sim N\\]\\(b_1\\), estimate intercept \\(\\beta_0\\) straightforward:\\[\\begin{align}\nb_0 = \\bar{y}- b_1\\bar{x}\\\\\n\\bar{y} = b_0 + b_1\\bar{x}\n\\end{align}\\]point \\((\\bar{x}, \\bar{y})\\) falls along regression line.","code":""},{"path":"simple-linear-regression.html","id":"linear-combinations","chapter":"1 Simple Linear Regression","heading":"1.6.3 Linear Combinations","text":"linear combination set values \\(a_1...a_n\\) formula:\\[\n\\sum_{=1}^n c_ia_i\n\\]\\(c_1...c_n\\) constants.Using notation can estimate \\(b_1\\):\\[\nb_1 = \\frac{\\sum(x_i-\\bar{x}) y_i}{\\sum(x_i-\\bar{x})^2}\n\\]factors exclusive \\(y_i\\) render constant \\(c_i\\).","code":""},{"path":"simple-linear-regression.html","id":"errors","chapter":"1 Simple Linear Regression","heading":"1.6.4 Errors","text":"Properties \\(\\epsilon_i\\):distribution errors, \\(\\epsilon_1...\\epsilon_n\\), normal.\\(\\sum e_i = 0\\)\\(\\sum y_i = \\sum \\hat y\\)\\(\\sum x_ie_i = 0\\)\\(\\sum y_ie_i = 0\\)SSE: Sum Squared ErrorsThe residual sum squares.$$\\[\\begin{align}\nSSE = \\sum_{= 1}^n e_i^2\\\\\n= \\sum_{= 1}^n (y_i - \\hat y)^2\n\n\\end{align}\\]$$MSE: Mean Squared Errors$$\\[\\begin{align}\nMSE = \\frac{\\sum_{= 1}^n (y_i - \\hat y)^2}{n-2} \\\\\n= \\frac{SSE}{n-2}\n\n\\end{align}\\]$$\\(n-2\\) degrees freedom estimating two parameters, \\(\\beta_0\\) \\(\\beta_1\\). MSE unbiased estimator \\(\\sigma^2\\), expected value true value.\\[ E(MSE) = \\sigma^2\\]estimates \\(\\beta\\)’s random variables. expect \\(b_1\\) change sampling. repeat sampling, expect MSE average \\(\\thicksim\\sigma^2\\).\\[\n\\sigma^2(b_1) = \\frac{\\sigma^2}{\\sum(x_i -\\bar x)^2}\n\\],\\[\ns^2(b_1)=\\frac{MSE}{\\sum(x_i -\\bar x)^2}\n\\]’ll use p-value confidence interval calculations.\\(H_0\\) true, \\(t^*\\) t-distribution \\(n-2\\) degrees freedom.Example want estimate systolic blood pressure 20 subjects. know following variables:\\(x\\): age\\(y\\): systolic blood pressure\\(\\beta_0\\): intercept, \\(90\\)\\(\\beta_1\\): slope, \\(0.9\\)Note \\(\\beta\\)’s usually unknown.given following information$$\\[\\begin{equation}\n\\sum_{=1}^{20} (x_1- \\bar x)(y_i - \\bar y)\\approx 4863.282\\\\\n\\sum_{=1}^{20} (x_1- \\bar x) \\approx 6072.55\\\\\n\\b_1 = \\frac{4863.282}{6072.55} \\approx 0.8\n\n\\end{equation}\\]$$\n:::","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"2 Hypothesis Testing","heading":"2 Hypothesis Testing","text":"three approaches two-sided hypothesis testing.three render conclusion use significance level, \\(\\alpha\\).P-valuesCritical Values (t-Testing)Confidence IntervalsGiven null (\\(H_0\\)) alternative (\\(H_1\\)) hypotheses:\\[\\begin{align}\nH_0: \\beta_1 = 0\\\\\nH_1: \\beta_1 \\neq 0\n\\end{align}\\]\\(b_1\\sim N\\) \\(E(b_1) = \\beta_1\\), can build test statistic, \\(t^*\\):\\[\nt^* = \\frac{b_1}{s(b_1)}\n\\]\\[\\begin{align}\ns(b_1) = \\sqrt{s^2(b_1)}\\\\\n= \\sqrt{\\frac{MSE}{\\sum(x_1-\\bar{x})^2}}\n\\end{align}\\]","code":""},{"path":"hypothesis-testing.html","id":"p-values","chapter":"2 Hypothesis Testing","heading":"2.1 P-values","text":"p-value \\(\\lt\\alpha\\), reject \\(H_0\\). Otherwise, reject \\(H_0\\).\\[ 2P(t \\leq |t^*|\\mid H_0 true)\\]","code":""},{"path":"hypothesis-testing.html","id":"critical-values","chapter":"2 Hypothesis Testing","heading":"2.2 Critical Values","text":"\\[\nt(1-\\frac{\\alpha}{2}; n-2)\n\\]","code":""}]
