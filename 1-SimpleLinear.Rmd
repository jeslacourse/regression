# Simple Linear Regression

> One predictor, one outcome

**Linear Regression:** The measure of lineal relationship between two variables. 

Simple regression will have one continuous response variable (y) and one continuous explanatory variable (x). Response variables are also known as the *dependent* or *outcome* variable. Explanatory variables, aka *independent*, *predictor*, or *covariate* variables, can include categorical values. 

$$y=\beta_0 + \beta_1x$$
Where $\beta_0$ is the y-intercept and $\beta_1$ is the slope of the function. 

## Modeling

$X$ is treated as a fixed variable whose values have been chosen by the researcher. Though it should be noted that regression is often used where variable is not wholly chosen. For example, sampling a population will return a distribution of ages that may not match the true distribution of the population. So $X$ is also, technically, random. 

$Y$ is a random variable 

Our regression model dependency of $Y$ on $X$
esti
$$Y=\beta_0 + \beta_1X + \epsilon$$
$\beta$'s and $X$ are considered fixed values, with random variable $\epsilon$.



Where $\beta_0$ and $\beta_1$ are parameters; usually unknown values related to the population and not the sample. 

- parameter: values for the population
- statistic: values for the sample 

> We are using statistics to estimate our parameters 

After collecting data from the population, we can create a sample on which to run our statistics for which we can estimate our parameters. 

|           | parameter | statistic |
|-----------|-----------|-----------|
| mean      | \mu       | \bar{y}   |
| variance  | \sigma^2  | s^2       |
| slope     | \beta_1   | b_1       |
| intercept | \beta_0   | b_0       |

## Utility

Regression is used for:

1. Observational studies
- just observe, no manipulation
- treatment is not randomized
2. Experimental studies
- manipulate the explanatory variables
- treatment must be randomized

(!) Correlations is not equal to causation in non-random studies 

## Expected Values

For a **discrete** random variable $Y$ with possible values $y_1... y_k$ we can say: 
$$ E(Y) = \Sigma_{i=1}^k y_1 P(Y = y_i)$$
where $E(Y)$ is a weighted average of the possible values $y_1... y_k$ and the weights $P(Y = y_1)...P(Y = y_k)$ are the expected probabilities. This will give us a weighted average.

Weighted Average:
$$\Sigma_{i=1}^k w_ia_i$$
If $\Sigma w_i = 1$ and $0 \leq w_1 \leq 1$. 


A **continuous** random variable $Y$ would be represented by a density function:

$$E(Y) = \int_{-\infty}^{\infty} yf(y)dy$$
For example, the normal distribution `Y ~ N(0,1)` with mean `0` and variance `1`, would be represented using the density function:

$$E(Y) = \int_{-\infty}^{\infty}y\frac{1}{\sqrt{2\pi}} 
  \exp\left( -\frac{y^2}{2}\right) dy = 0 $$
  
The **population mean** is the expected value. If $Z_1..Z_N$ for all values in the population, then we would use the parameter: 

$$ E(Z) = \frac{1}{N}\Sigma_{i=1}^N Z_i$$

**Sample means** are averages. Given the sample data $X_1... X_n$, would return the *statistic*:

$$ \bar{x}  = \frac{1}{n} \Sigma_{i = 1}^nX_i$$

### Rules for Expectation 

Given random variables $X$ and $Y$ and constants $a,b,$ & $c$: 

1. $E(c) = c$
2. $E(cX) = cE(X)$
3. $E(X+Y) = E(X) + E(Y)$


An example: 
$$E(a+bX+cY) \\ = a + bE(X) + cE(Y)$$

## Variance

We can use the **population variance** formula to show that $E(Y)$ is equivalent to the parameter $\mu$. Given the population variance $var(Y)$: 


$$ var(Y) = E[Y-E(Y)]^2\\=E[Y-\mu]^2\\\therefore \mu = E(Y)$$

The squaring of $[Y-E(Y)]$ "removes the sign", reverting all distances to positive distances from the mean.  

Given our known substitutions, we can say:

$$ var(Y) = E[Y-E(Y)]^2\\=\Sigma_{i=1}^k (y_i-\mu)^2P(Y-y_i)$$

If $Z_1... Z_n$ account for all values in a population, we would have parameter:

$$var(Z) = \frac{1}{N}\Sigma_{i=1}^{N}(Z_i-\bar{Z})^2$$

where $\bar{Z} = E(Z)$. 


Sample data $x_i...x_n$ would give us the **unbiased sample variance**:

$$s^2 = \frac{\Sigma(x_i - \bar{x})^2}{n-1}$$

Focusing on the denominator, sample sizes $n$ will shift the statistic significantly compared to if the sample was larger. 

### Rules for Variance

1. $var(a) = 0$ 
2. $var(aX) = a^2 var(X)$
3. $var(X+Y)  = var(X) + var(Y) + 2cov(X,Y)$ 

An example:

$$var(a+bX-cY) \\= var(a) + var(bX-cY) + 2cov(X,Y)\\= 0 + var(bX)+ var(-cY) \\=b^2var(X) + c^2var(Y)+2[(-bc)cov(X,Y)]$$


## Bias

Because of random variability, $s^2$ will vary with each experiment. An unbiased sample variance is an average value of those experiments. 

$$ E(s^2) = E\frac{\Sigma(Y_i-\bar{Y})^2}{n-1}= var(Y)$$ 

## Covariance 

Given the random variables $(X,Y)$ **covariance** is defined as: 

$$cov(X,Y) = E[(X -\mu_x)(Y-\mu_y)]$$

with **sample covariance**

$$\hat{cov}(X,Y) = \frac{\Sigma(X_i - \bar{X})(Y_i-\bar{Y})^2}{n-1} $$
### Rules for Covariance 

1. $cov(X,Y) = cov(Y,X)$
2. if $X тлл Y \Rightarrow cov(X,Y) = 0$, if the random variables are independent of one another, then we cannot predict their variability
3. if $cov(X,Y)= 0 \Rightarrow$ may or may not $X тлл Y$
4. $cov(a,b) = 0$
5. $cov(a, X) = 0$ 
6. $cov(aX, Y) = a cov(X,Y)$ 
7. $cov(aX, bY) = ab cov(X,Y)$
8. $cov(X, Y+Z) = cov(X,Y)+cov(X,Z)$
9. $cov(X,X) = var(X)$

## Correlation 

**Correlation** is defined on the scale $-1 \leq corr(X,Y) \leq 1$ where: 

$$corr(X,Y) = \frac{cov(X,Y)}{\sqrt{var(X)var(Y)}}$$






## Modeling Simple Linear Regression  

$$Y_i= \beta_0 +\beta_1X_i + \epsilon_i$$

where:

- $Y_i$: random variable for experimental unit
- $X_i$: predictor for experimental unit, a fixed value
- $\beta_0, \beta_1$: parameters, usually known. The goal is to estimate them. 

The model is considered **simple** because we are using one predictor. The model is linear in parameters and linear in the predictor variable, $X$. 

Properties for $\epsilon_i$: 
1. $E(\epsilon_i) = 0$, $\forall i$
2. $var(\epsilon_i)$ is constant
3. $\epsilon_i...\epsilon_n$ are uncorrelated

On average, the error *is* the regression line, 0. Put another way, 
 
 > The expected value of $\epsilon$ is on the regression line. 
 
 1. 
 
 $$ E(\epsilon_i) = 0\\
 \epsilon_i = Y_i -(\beta_0 +\beta_1X_i) \\
 E[\epsilon_i] = E[Y_i -(\beta_0 +\beta_1X_i)] \\
 0 = E[Y_i] - E[\beta_0 +\beta_1X_i] \\
 0 = E[Y_i] - E[\beta_0] +E[\beta_1X_i] \\
 0  = E[Y_i] - \beta_0 + \beta_1X_i \\
   \Rightarrow E[Y_i] = \beta_0 + \beta_1X_i \\
 $$

showing that on average, $Y_i$ falls along the regression line. We can say the regression lineis the average of $Y_i$, conditional on $X_i$. 

$$ E(Y_i|X_i)= \beta_0 +\beta_1X_i $$
 2. $\epsilon_i...\epsilon_n$ have the same variance 
 
 Define $\sigma^2(\epsilon_i) \equiv \sigma^2 $,  then $\sigma^2(Y_i) = var(Y_i) $. $var(\epsilon_i)$ increases as $X$ increases. 
 
 3. $\epsilon_i...\epsilon_n$ are uncorrelated 
 
 $$
  corr(\epsilon_i,\epsilon_j) = 0, i\neq j \\ 
  corr(\epsilon_i,\epsilon_j) = var(\epsilon_i) \neq 0 \\
  cov(\epsilon_i,\epsilon_j) = 0, i\neq j 
 $$
 
Similarly, but with different notation, $corr(\epsilon_i,\epsilon_j) = \sigma(\epsilon_i,\epsilon_j)$. 
 
### Features of Models

1. $Y_i$ is a sum of a constant term plus a random variable. The constant in this case is $\beta_0 +\beta_1X_i$. The random variable is $\epsilon_i$. 

2. $E(Y_i|X_i)= \beta_0 +\beta_1X_i$, see $\epsilon$ property 2. 

3. $\epsilon_i = Y_i -(\beta_0 +\beta_1X_i)$ is a random deviation of $Y_i$ around the regression line.

It's worth taking a moment to make the distinction between the regression model, which uses subscripts $i$:  
$$Y_i= \beta_0 +\beta_1X_i + \epsilon_i$$
and the regression function: 

$$Y= \beta_0 +\beta_1X + \epsilon$$
An example: 

We want to estimate the systolic blood pressure for a 20 year old. We know the following variables: 

- $x$: age
- $y$: systolic blood pressure
- $\beta_0$: the intercept, $90$
- $\beta_1$: the slope, $0.9$

Note that $\beta$'s are usually unknown. We can say, we expect systolic blood pressure to be $90$ at age $0$ (or at birth), increasing $0.9$ units every year.  

$$
E(Y|X = 20) \\= 90 + 0.9(20)\\=108
$$

Factoring in an error margin, given $X = 20$ we would expect $Y = 108 +\epsilon$. 


## Least Squares Estimation

### Estimating $\beta$'s

Find "good" estimates of $\beta_0$ and $\beta_1$ using the **least squares method**.  

When you have $n$ pairs of data, $(x_1, y_1)...(x_n, y_n)$: 

Use the **deviation** formula

$$e_i = y_i - (\beta_0 +\beta_1x_i)$$ 

to compose the **squared deviation** formula: 

$$e_i^2 = [y_i - (\beta_0 +\beta_1x_i)]^2$$ 

This will allow us to define $Q$, the **sum of squared deviations**: 

$$
Q \equiv \Sigma_{i = 1}^n e_i^2\\=\Sigma_{i = 1}^n [y_i - (\beta_0 +\beta_1x_i)]^2
$$

The least squares estimates are values of $\beta_0$ and $\beta_1$ that minimize $Q$. 

We use $b_1$ to estimate the slope $\beta_1$:

$$
 b_1 = \frac{\Sigma(x_i-\bar{x})(y_i-\bar{y})}{\Sigma(x_i-\bar{x})^2}\\ 
 =\frac{\hat{cov}(X,Y)}{\hat{var}(X,Y)} 
 
$$

Once we have $b_1$, the estimate for the intercept $\beta_0$ is straightforward:



$$
b_0 = \bar{y}- b_1\bar{x}\\
 \bar{y} = b_0 + b_1\bar{x}
$$

where the point $(\bar{x}, \bar{y})$ falls along the regression line. 

**Properties** 

The estimators $b_0$ and $b_1$ are unbiased estimators. They only require that for any $\epsilon_i$, that $E(\epsilon_i) = 0$.
